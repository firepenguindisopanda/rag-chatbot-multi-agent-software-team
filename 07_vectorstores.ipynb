{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qk4Uw_iSr3Mc",
   "metadata": {
    "id": "Qk4Uw_iSr3Mc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 7:** Retrieval-Augmented Generation with Vector Stores</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the previous notebook, we learned about embedding models and exercised some of their capabilities. We discussed their intended use cases of longer-form document comparison and found ways to use it as a backbone for more custom semantic comparisons. This notebook will progress these ideas toward the retrieval model's intended use case and explore how to build chatbot systems that rely on *vector stores* to automatically save and retrieve information.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Understand how semantic-similarity-backed systems can facilitate easy-to-use retrieval formulations.\n",
    "\n",
    "- Learn how to incorporate retrieval modules into your chat model systems for a retrieval-augmented generation (RAG) pipeline, which can be applied to tasks like document retrieval and conversation memory buffers.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- This notebook does not attempt to incorporate hierarchical reasoning or non-naive RAG (such as planning agents). Consider what modifications would be necessary to make these components work in an LCEL chain.\n",
    "\n",
    "- Consider when it would be best to move your vector store solution into a scalable service and when a GPU will become necessary for optimization.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5XmeiiOWtuxC",
   "metadata": {
    "id": "5XmeiiOWtuxC"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37fe234-2bdb-4107-8483-efda9aa5e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## Part 1: Summary of RAG Workflows\n",
    "\n",
    "This notebook will explore several paradigms and derive reference code to help you approach some of the most common retrieval-augmented workflows. Specifically, the following sections will be covered (with the differences highlighted):\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***Vector Store Workflow for Conversational Exchanges:***\n",
    "- Generate semantic embedding for each new conversation.\n",
    "- Add the message body to a vector store for retrieval.\n",
    "- Query the vector store for relevant messages to fill in the LLM context.\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***Modified Workflow for an Arbitrary Document:***\n",
    "- **Divide the document into chunks and process them into useful messages.**\n",
    "- Generate semantic embedding for each **new document chunk**.\n",
    "- Add the **chunk bodies** to a vector store for retrieval.\n",
    "- Query the vector store for relevant **chunks** to fill in the LLM context.\n",
    "    - ***Optional:* Modify/synthesize results for better LLM results.**\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Extended Workflow for a Directory of Arbitrary Documents:**\n",
    "- Divide **each document** into chunks and process them into useful messages.\n",
    "- Generate semantic embedding for each new document chunk.\n",
    "- Add the chunk bodies to **a scalable vector database for fast retrieval**.\n",
    "    - ***Optional*: Exploit hierarchical or metadata structures for larger systems.**\n",
    "- Query the **vector database** for relevant chunks to fill in the LLM context.\n",
    "    - *Optional:* Modify/synthesize results for better LLM results.\n",
    "\n",
    "<br>\n",
    "\n",
    "Some of the most important terminology surrounding RAG is covered in detail on the [**LlamaIndex Concepts page**](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html), which itself is a great starting point for progressing towards the LlamaIndex loading and retrieving strategy. We highly recommend using it as a reference as you continue with this notebook and advise you to try out LlamaIndex after the course to consider the pros and cons firsthand!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437",
   "metadata": {
    "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437"
   },
   "source": [
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1cFbKbVvLLnFPs3yWCKIuzXkhBWh6nLQY\" width=1200px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Retrieval | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/v0.1/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XaZ20XoeSTD-",
   "metadata": {
    "id": "XaZ20XoeSTD-"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** RAG for Conversation History\n",
    "\n",
    "In our previous explorations, we delved into the capabilities of document embedding models and used them to embed, store, and compare semantic vector representations of text. Though we could motivate how to efficiently extend this into vector store land manually, the true beauty of working with a standard API is its strong incorporation with other frameworks that can already do the heavy lifting for us!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LRx0XUf_Sdxw",
   "metadata": {
    "id": "LRx0XUf_Sdxw"
   },
   "source": [
    "### **Step 1**: Getting A Conversation\n",
    "\n",
    "Consider a conversation crafted using Llama-13B between a chat agent and a blue bear named Beras. This dialogue, dense with details and potential diversions, provides a rich dataset for our study:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "IUfCuMkoShWI",
   "metadata": {
    "id": "IUfCuMkoShWI"
   },
   "outputs": [],
   "source": [
    "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
    "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
    "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
    "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
    "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
    "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
    "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
    "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
    "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tDL2tAo2Skh2",
   "metadata": {
    "id": "tDL2tAo2Skh2"
   },
   "source": [
    "Using the manual embedding strategy from the previous notebook is still very viable, but we can also rest easy and let a **vector store** do all that work for us!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hIp943mSqGZ",
   "metadata": {
    "id": "5hIp943mSqGZ"
   },
   "source": [
    "### **Step 2:** Constructing Our Vector Store Retriever\n",
    "\n",
    "To streamline similarity queries on our conversation, we can employ a vector store to help keep track of passages for us! **Vector Stores**, or vector storage systems, abstract away most of the low-level details of the embedding/comparison strategies and provide a simple interface to load and compare vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pnaOBgexS-kp",
   "metadata": {
    "id": "pnaOBgexS-kp"
   },
   "source": [
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1ZjwYbSZzsXK6ZP8O1-cY3BeRffV4oqzb\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/vector_stores.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Vector Stores | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DwZUh6kgS5Ki",
   "metadata": {
    "id": "DwZUh6kgS5Ki"
   },
   "source": [
    "<br>\n",
    "\n",
    "In addition to simplifying the process from an API perspective, vector stores also implement connectors, integrations, and optimizations under the hood. In our case, we will start with the [**FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss), which integrates a LangChain-compatable Embedding model with the [**FAISS (Facebook AI Similarity Search)**](https://github.com/facebookresearch/faiss) library to make the process fast and scalable on our local machine!\n",
    "\n",
    "**Specifically:**\n",
    "\n",
    "1. We can feed our conversation into [**a FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss) via the `from_texts` constructor. This will take our conversational data and the embedding model to create a searchable index over our discussion.\n",
    "2. This vector store can then be \"interpreted\" as a retriever, supporting the LangChain runnable API and returning documents retrieved via an input query.\n",
    "\n",
    "The following shows how you can construct a FAISS vector store and reinterpret it as a retriever using the LangChain `vectorstore` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1kE2-ejoTKKU",
   "metadata": {
    "id": "1kE2-ejoTKKU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 227 ms, sys: 348 ms, total: 576 ms\n",
      "Wall time: 813 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Streamlined from_texts FAISS vectorstore construction from text list\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "retriever = convstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muN66v5PW5dW",
   "metadata": {
    "id": "muN66v5PW5dW"
   },
   "source": [
    "The retriever can now be used like any other LangChain runnable to query the vector store for some relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "kNZJTnlEWVYh",
   "metadata": {
    "id": "kNZJTnlEWVYh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'eb30446c-3358-408c-8eb1-d77039b70790'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'31673cf2-5db1-476f-b32f-4438033c8b42'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'20e2dc61-2197-4ac0-9e68-e6ba727d7e6f'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you![Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'0cfa8b27-4aaa-413a-9d5e-98de970338f0'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'eb30446c-3358-408c-8eb1-d77039b70790'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'31673cf2-5db1-476f-b32f-4438033c8b42'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'20e2dc61-2197-4ac0-9e68-e6ba727d7e6f'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I hope you get to visit them someday, Beras! It would be a great adventure for \u001b[0m\n",
       "\u001b[32myou!\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Thank you for the suggestion! Ill definitely keep it in mind for the future.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'0cfa8b27-4aaa-413a-9d5e-98de970338f0'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "SE1eDZTEWScC",
   "metadata": {
    "id": "SE1eDZTEWScC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'1d18ab35-02b1-4f08-83c8-8a562fd96513'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'0cfa8b27-4aaa-413a-9d5e-98de970338f0'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'eb30446c-3358-408c-8eb1-d77039b70790'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'31673cf2-5db1-476f-b32f-4438033c8b42'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'1d18ab35-02b1-4f08-83c8-8a562fd96513'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
       "\u001b[32macross North America'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'0cfa8b27-4aaa-413a-9d5e-98de970338f0'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'eb30446c-3358-408c-8eb1-d77039b70790'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'31673cf2-5db1-476f-b32f-4438033c8b42'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtNCEXLYTVf4",
   "metadata": {
    "id": "mtNCEXLYTVf4"
   },
   "source": [
    "As we can see, our retriever found a handful of semantically relevant documents from our query. You may notice that not all of the documents are useful or clear on their own. For example, a retrieval of *\"Beras\"* for *\"your name\"* may be problematic for the chatbot if provided out of context. Anticipating the potential problems and creating synergies between your LLM components can increase the likelihood of good RAG behavior, so keep an eye out for such pitfalls and opportunities.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZEDEzpqmTYMv",
   "metadata": {
    "id": "ZEDEzpqmTYMv"
   },
   "source": [
    "### **Step 3:** Incorporating Conversation Retrieval Into Our Chain\n",
    "\n",
    "Now that we have our loaded retriever component as a chain, we can incorporate it into our existing chat system as before. Specifically, we can start with an ***always-on RAG formulation*** where:\n",
    "- **A retriever is always retrieving context by default**.\n",
    "- **A generator is acting on the retrieved context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64abe478-9bcb-4802-a26e-dc5a1756e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "uue5UY3_TcvF",
   "metadata": {
    "id": "uue5UY3_TcvF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It seems that Beras lives in the Arctic. The cooler climate there is quite different from the warm climate found in</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the Rocky Mountains, which Beras mentioned was one of the reasons for his curiosity about the mountain range.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mIt seems that Beras lives in the Arctic. The cooler climate there is quite different from the warm climate found in\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe Rocky Mountains, which Beras mentioned was one of the reasons for his curiosity about the mountain range.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {question}\"\n",
    "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    # | RPrint()\n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(chain.invoke(\"Where does Beras live?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FSIqTMuuTjIh",
   "metadata": {
    "id": "FSIqTMuuTjIh"
   },
   "source": [
    "Take a second to try out some more invocations and see how the new setup performs. Regardless of your model choice, the following questions should serve as interesting starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4jDJwrYpTmpd",
   "metadata": {
    "id": "4jDJwrYpTmpd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">without more context, I can't give you a precise location beyond that. However, if you're interested, you can </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">always look up more information online or watch documentaries about the Rocky Mountains to learn more!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwithout more context, I can't give you a precise location beyond that. However, if you're interested, you can \u001b[0m\n",
       "\u001b[1;38;2;118;185;0malways look up more information online or watch documentaries about the Rocky Mountains to learn more!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "-artagLfTpBy",
   "metadata": {
    "id": "-artagLfTpBy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that stretch across North America. To answer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">your question about their location, they primarily run through the western part of the continent, from northern </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">British Columbia in Canada, all the way down to New Mexico in the United States.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As for your question about California, the Rocky Mountains do not pass directly through California. However, they </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">do border California to the east, as they run through the states of Idaho and Nevada. I hope that helps! Would you </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like to learn more about the Rocky Mountains?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that stretch across North America. To answer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0myour question about their location, they primarily run through the western part of the continent, from northern \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mBritish Columbia in Canada, all the way down to New Mexico in the United States.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAs for your question about California, the Rocky Mountains do not pass directly through California. However, they \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdo border California to the east, as they run through the states of Idaho and Nevada. I hope that helps! Would you \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike to learn more about the Rocky Mountains?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "GDgjdfdpTrV5",
   "metadata": {
    "id": "GDgjdfdpTrV5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Unfortunately, the context doesn't provide information on Beras's exact location or the distance from the Rocky </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mountains. However, since Beras mentioned living in the arctic, it's safe to say that he's quite far from the Rocky</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mountains. The Rocky Mountains are located in North America, with the majority of the range being in the United </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">States and Canada, while the Arctic region is at the northernmost part of the world.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mUnfortunately, the context doesn't provide information on Beras's exact location or the distance from the Rocky \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMountains. However, since Beras mentioned living in the arctic, it's safe to say that he's quite far from the Rocky\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMountains. The Rocky Mountains are located in North America, with the majority of the range being in the United \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mStates and Canada, while the Arctic region is at the northernmost part of the world.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wp9-8CbT0L9",
   "metadata": {
    "id": "8wp9-8CbT0L9"
   },
   "source": [
    "<br>\n",
    "\n",
    "You might notice some decent performance with this always-on retrieval node in the loop since the actual context being fed into the LLM remains relatively small. It's important to experiment with factors like embedding sizes, context limits, and model options to see what kinds of behavior you can expect and which efforts are worth taking to improve performance.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OnpOybOhUCTf",
   "metadata": {
    "id": "OnpOybOhUCTf"
   },
   "source": [
    "### **Step 4:** Automatic Conversation Storage\n",
    "\n",
    "Now that we see how our vector store memory unit should function, we can perform one last integration to allow our conversation to add new entries to our conversation: a runnable that calls the `add_texts` method for us to update the store state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "FsK6-AtRVdcZ",
   "metadata": {
    "id": "FsK6-AtRVdcZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying some ice cream, the context </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">only mentions the scenic beauty and significance of the Rockies, not any food options. However, regardless of where</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">you are, ice cream is always a delightful treat, isn't it? We could perhaps discuss the best spots for viewing the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">mountains while savoring a scoop or two. What do you think?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWhile I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying some ice cream, the context \u001b[0m\n",
       "\u001b[1;38;2;118;185;0monly mentions the scenic beauty and significance of the Rockies, not any food options. However, regardless of where\u001b[0m\n",
       "\u001b[1;38;2;118;185;0myou are, ice cream is always a delightful treat, isn't it? We could perhaps discuss the best spots for viewing the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmountains while savoring a scoop or two. What do you think?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Well, based on our conversation, I'd hazard a guess that you might have a fondness for ice cream! It certainly </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">seems like a treat you're excited about. But remember, this is just a speculation based on our chat. Everyone's </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">taste buds are different, and that's what makes food such an interesting topic!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWell, based on our conversation, I'd hazard a guess that you might have a fondness for ice cream! It certainly \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mseems like a treat you're excited about. But remember, this is just a speculation based on our chat. Everyone's \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtaste buds are different, and that's what makes food such an interesting topic!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">I apologize for the misunderstanding! It's fascinating to know that your favorite food is actually honey, not ice </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">cream as I initially speculated. Everyone's unique taste preferences are what make our culinary conversations so </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">engaging! If you'd like, we can explore the Rockies while enjoying some honey-flavored treats. Doesn't that sound </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like a sweet adventure?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mI apologize for the misunderstanding! It's fascinating to know that your favorite food is actually honey, not ice \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcream as I initially speculated. Everyone's unique taste preferences are what make our culinary conversations so \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mengaging! If you'd like, we can explore the Rockies while enjoying some honey-flavored treats. Doesn't that sound \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike a sweet adventure?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">I sure do! After our little chat, I can confidently say that your favorite food is honey. Isn't it fascinating how </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">much we can learn from each other during our conversations? And, speaking of honey, can you imagine exploring the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Rockies while savoring some honey-infused treats? That would indeed be a sweet adventure!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mI sure do! After our little chat, I can confidently say that your favorite food is honey. Isn't it fascinating how \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmuch we can learn from each other during our conversations? And, speaking of honey, can you imagine exploring the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mRockies while savoring some honey-infused treats? That would indeed be a sweet adventure!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Reset knowledge base and define what it means to add more messages.\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
    "    return d.get('output')\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    "    \"\\nAnswer the user conversationally. Make sure the conversation flows naturally.\\n\"\n",
    "    \"[Agent]\"\n",
    ")\n",
    "\n",
    "\n",
    "conv_chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'input': (lambda x:x)\n",
    "    }\n",
    "    | RunnableAssign({'output' : chat_prompt | instruct_llm | StrOutputParser()})\n",
    "    | partial(save_memory_and_get_output, vstore=convstore)\n",
    ")\n",
    "\n",
    "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Actually, my favorite is honey! Not sure where you got that idea?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KRMW6G7NVSWF",
   "metadata": {
    "id": "KRMW6G7NVSWF"
   },
   "source": [
    "Unlike the more automatic full-text or rule-based approaches to injecting context into the LLM, this approach ensures some amount of consolidation which can keep the context length from getting out of hand. It's still not a full-proof strategy on its own, but it's a stark improvement for unstructured conversations (and doesn't even require a strong instruction-tuned model to perform slot-filling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9TPkh3SaLbqh",
   "metadata": {
    "id": "9TPkh3SaLbqh"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3 [Exercise]:** RAG For Document Chunk Retrieval\n",
    "\n",
    "Given our prior exploration of document loading, the idea that data chunks can be embedded and searched through probably isn't surprising. With that said, it is definitely worth going over since applying RAG with documents is a double-edged sword; it may **seem** to work well out of the box but requires some extra care when optimizing it for truly reliable performance. It also provides an excellent opportunity to review some fundamental LCEL skills, so let's see what we can do!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Exercise:**\n",
    "\n",
    "In the previous example, you may recall that we pulled in some relatively small papers with the help of [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv) using the following syntax:\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = [\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct\n",
    "]\n",
    "```\n",
    "\n",
    "Given all that you've learned so far, choose a selection of papers that you would like to use and develop a chatbot that can talk about them!\n",
    "\n",
    "<br>\n",
    "\n",
    "Though this is a pretty big task, a walkthrough of ***most*** of the process will be provided below. By the end of the walkthrough, many of the necessary puzzle pieces will be provided, and your real task will be to integrate them together for the final `retrieval_chain`. When you're done, get ready to re-integrate the chain (or a flavor of your choice) in the last notebook as part of the evaluation exercise!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSjfCtiQnj9e",
   "metadata": {
    "id": "jSjfCtiQnj9e"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1**: Loading And Chunking Your Documents\n",
    "\n",
    "The following code block gives you some default papers to load in for your RAG chain. Feel free to select more papers as desired, but note that longer documents will take longer to process. A few simplifying assumptions and additional processing steps are included to help you improve your naive RAG performance:\n",
    "\n",
    "- Documents are cut off prior to the \"References\" section if one exists. This will keep our system from considering the citations and appendix sections, which tend to be long and distracting.\n",
    "\n",
    "- A chunk that lists the available documents is inserted to provide a high-level view of all available documents in a single chunk. If your pipeline does not provide metadata on each retrieval, this is a useful component and can even be listed among a list of higher-priority pieces if appropriate.\n",
    "\n",
    "- Additionally, the metadata entries are also inserted to provide general information. Ideally, there would also be some synthetic chunks that merge the metadata into interesting cross-document chunks.\n",
    "\n",
    "**NOTE:** ***For the sake of the assessment, please include at least one paper that is less than one month old!***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "S-3FBdT_lhVT",
   "metadata": {
    "id": "S-3FBdT_lhVT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Documents\n",
      "Chunking Documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - ReAct: Synergizing Reasoning and Acting in Language Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - High-Resolution Image Synthesis with Latent Diffusion Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Learning Transferable Visual Models From Natural Language Supervision</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Mistral 7B\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - ReAct: Synergizing Reasoning and Acting in Language Models\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - High-Resolution Image Synthesis with Latent Diffusion Models\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Learning Transferable Visual Models From Natural Language Supervision\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      " - # Chunks: 35\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-02'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kaiser, Illia Polosukhin'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constituency parsing both with large and limited training data.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-08-02'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Attention Is All You Need'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz \u001b[0m\n",
       "\u001b[32mKaiser, Illia Polosukhin'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural \u001b[0m\n",
       "\u001b[32mnetworks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder \u001b[0m\n",
       "\u001b[32mthrough an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on \u001b[0m\n",
       "\u001b[32mattention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation\u001b[0m\n",
       "\u001b[32mtasks show these models to be\\nsuperior in quality while being more parallelizable and requiring \u001b[0m\n",
       "\u001b[32msignificantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation \u001b[0m\n",
       "\u001b[32mtask, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 \u001b[0m\n",
       "\u001b[32mEnglish-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 \u001b[0m\n",
       "\u001b[32mafter training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the \u001b[0m\n",
       "\u001b[32mliterature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish \u001b[0m\n",
       "\u001b[32mconstituency parsing both with large and limited training data.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1\n",
      " - # Chunks: 45\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2019-05-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2019-05-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional \u001b[0m\n",
       "\u001b[32mEncoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to \u001b[0m\n",
       "\u001b[32mpre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right \u001b[0m\n",
       "\u001b[32mcontext in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output \u001b[0m\n",
       "\u001b[32mlayer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language \u001b[0m\n",
       "\u001b[32minference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and \u001b[0m\n",
       "\u001b[32mempirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, \u001b[0m\n",
       "\u001b[32mincluding\\npushing the GLUE score to 80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, MultiNLI\\naccuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% \u001b[0m\n",
       "\u001b[32mabsolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 question answering\\nTest F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD \u001b[0m\n",
       "\u001b[32mv2.0 Test F1 to 83.1\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 2\n",
      " - # Chunks: 46\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-04-12'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-04-12'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001b[0m\n",
       "\u001b[32mHeinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, \u001b[0m\n",
       "\u001b[32mand achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and\u001b[0m\n",
       "\u001b[32mprecisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags \u001b[0m\n",
       "\u001b[32mbehind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their \u001b[0m\n",
       "\u001b[32mworld knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism \u001b[0m\n",
       "\u001b[32mto\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive \u001b[0m\n",
       "\u001b[32mdownstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -- \u001b[0m\n",
       "\u001b[32mmodels which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG \u001b[0m\n",
       "\u001b[32mmodels where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector \u001b[0m\n",
       "\u001b[32mindex\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which \u001b[0m\n",
       "\u001b[32mconditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different \u001b[0m\n",
       "\u001b[32mpassages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set\u001b[0m\n",
       "\u001b[32mthe state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific \u001b[0m\n",
       "\u001b[32mretrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more \u001b[0m\n",
       "\u001b[32mspecific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 3\n",
      " - # Chunks: 40\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-05-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge sources and discrete reasoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Amnon Shashua, Moshe Tenenholtz'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-05-01'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external \u001b[0m\n",
       "\u001b[32mknowledge sources and discrete reasoning'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\u001b[0m\n",
       "\u001b[32mBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, \u001b[0m\n",
       "\u001b[32mAmnon Shashua, Moshe Tenenholtz'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Huge language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have ushered in a new era for AI, serving as a\\ngateway to \u001b[0m\n",
       "\u001b[32mnatural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently \u001b[0m\n",
       "\u001b[32mlimited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a \u001b[0m\n",
       "\u001b[32msystems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to \u001b[0m\n",
       "\u001b[32mlinguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete \u001b[0m\n",
       "\u001b[32mknowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, \u001b[0m\n",
       "\u001b[32mKnowledge and Language \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMRKL, pronounced \"miracle\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m system,\\nsome of the technical challenges in implementing it, \u001b[0m\n",
       "\u001b[32mand Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 4\n",
      " - # Chunks: 21\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-10-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">under the Apache 2.0 license.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-10-10'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Mistral 7B'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
       "\u001b[32mDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, \u001b[0m\n",
       "\u001b[32mMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior \u001b[0m\n",
       "\u001b[32mperformance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in\u001b[0m\n",
       "\u001b[32mreasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for \u001b[0m\n",
       "\u001b[32mfaster\\ninference, coupled with sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to effectively handle\\nsequences of arbitrary length\u001b[0m\n",
       "\u001b[32mwith a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, \u001b[0m\n",
       "\u001b[32mthat surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released \u001b[0m\n",
       "\u001b[32munder the Apache 2.0 license.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 5\n",
      " - # Chunks: 44\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversations with\\nhuman preferences are publicly available </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-12-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, \u001b[0m\n",
       "\u001b[32mZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Evaluating large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m based chat assistants is challenging\\ndue to their broad \u001b[0m\n",
       "\u001b[32mcapabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore\u001b[0m\n",
       "\u001b[32musing strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and \u001b[0m\n",
       "\u001b[32mlimitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited \u001b[0m\n",
       "\u001b[32mreasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between \u001b[0m\n",
       "\u001b[32mLLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot \u001b[0m\n",
       "\u001b[32mArena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both \u001b[0m\n",
       "\u001b[32mcontrolled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement \u001b[0m\n",
       "\u001b[32mbetween humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which \u001b[0m\n",
       "\u001b[32mare otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement\u001b[0m\n",
       "\u001b[32meach other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K \u001b[0m\n",
       "\u001b[32mconversations with\\nhuman preferences are publicly available \u001b[0m\n",
       "\u001b[32mat\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 6\n",
      " - # Chunks: 123\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-03-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'ReAct: Synergizing Reasoning and Acting in Language Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-03-10'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'ReAct: Synergizing Reasoning and Acting in Language Models'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'While large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have demonstrated impressive capabilities\\nacross tasks in \u001b[0m\n",
       "\u001b[32mlanguage understanding and interactive decision making, their\\nabilities for reasoning \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g. chain-of-thought \u001b[0m\n",
       "\u001b[32mprompting\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and acting \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g.\\naction plan generation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have primarily been studied as separate topics. In \u001b[0m\n",
       "\u001b[32mthis\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an \u001b[0m\n",
       "\u001b[32minterleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, \u001b[0m\n",
       "\u001b[32mand update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, \u001b[0m\n",
       "\u001b[32msuch as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to \u001b[0m\n",
       "\u001b[32ma diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art \u001b[0m\n",
       "\u001b[32mbaselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting\u001b[0m\n",
       "\u001b[32mcomponents.\\nConcretely, on question answering \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotpotQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and fact verification \u001b[0m\u001b[32m(\u001b[0m\u001b[32mFever\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\nReAct overcomes issues of\u001b[0m\n",
       "\u001b[32mhallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia\u001b[0m\n",
       "\u001b[32mAPI, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without \u001b[0m\n",
       "\u001b[32mreasoning traces. On two interactive decision making\\nbenchmarks \u001b[0m\u001b[32m(\u001b[0m\u001b[32mALFWorld and WebShop\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ReAct outperforms \u001b[0m\n",
       "\u001b[32mimitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being\u001b[0m\n",
       "\u001b[32mprompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 7\n",
      " - # Chunks: 52\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-04-13'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'High-Resolution Image Synthesis with Latent Diffusion Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj√∂rn Ommer'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'By decomposing the image formation process into a sequential application of\\ndenoising </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">autoencoders, diffusion models (DMs) achieve state-of-the-art\\nsynthesis results on image data and beyond. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Additionally, their formulation\\nallows for a guiding mechanism to control the image generation process </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">without\\nretraining. However, since these models typically operate directly in pixel\\nspace, optimization of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">powerful DMs often consumes hundreds of GPU days and\\ninference is expensive due to sequential evaluations. To </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enable DM training on\\nlimited computational resources while retaining their quality and flexibility,\\nwe apply </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">them in the latent space of powerful pretrained autoencoders. In\\ncontrast to previous work, training diffusion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models on such a representation\\nallows for the first time to reach a near-optimal point between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">complexity\\nreduction and detail preservation, greatly boosting visual fidelity. By\\nintroducing cross-attention </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layers into the model architecture, we turn\\ndiffusion models into powerful and flexible generators for general </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditioning\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\npossible in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">convolutional manner. Our latent diffusion models (LDMs) achieve\\na new state of the art for image inpainting and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">highly competitive performance\\non various tasks, including unconditional image generation, semantic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scene\\nsynthesis, and super-resolution, while significantly reducing computational\\nrequirements compared to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pixel-based DMs. Code is available at\\nhttps://github.com/CompVis/latent-diffusion .'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-04-13'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'High-Resolution Image Synthesis with Latent Diffusion Models'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj√∂rn Ommer'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'By decomposing the image formation process into a sequential application of\\ndenoising \u001b[0m\n",
       "\u001b[32mautoencoders, diffusion models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m achieve state-of-the-art\\nsynthesis results on image data and beyond. \u001b[0m\n",
       "\u001b[32mAdditionally, their formulation\\nallows for a guiding mechanism to control the image generation process \u001b[0m\n",
       "\u001b[32mwithout\\nretraining. However, since these models typically operate directly in pixel\\nspace, optimization of \u001b[0m\n",
       "\u001b[32mpowerful DMs often consumes hundreds of GPU days and\\ninference is expensive due to sequential evaluations. To \u001b[0m\n",
       "\u001b[32menable DM training on\\nlimited computational resources while retaining their quality and flexibility,\\nwe apply \u001b[0m\n",
       "\u001b[32mthem in the latent space of powerful pretrained autoencoders. In\\ncontrast to previous work, training diffusion \u001b[0m\n",
       "\u001b[32mmodels on such a representation\\nallows for the first time to reach a near-optimal point between \u001b[0m\n",
       "\u001b[32mcomplexity\\nreduction and detail preservation, greatly boosting visual fidelity. By\\nintroducing cross-attention \u001b[0m\n",
       "\u001b[32mlayers into the model architecture, we turn\\ndiffusion models into powerful and flexible generators for general \u001b[0m\n",
       "\u001b[32mconditioning\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\npossible in a \u001b[0m\n",
       "\u001b[32mconvolutional manner. Our latent diffusion models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLDMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m achieve\\na new state of the art for image inpainting and \u001b[0m\n",
       "\u001b[32mhighly competitive performance\\non various tasks, including unconditional image generation, semantic \u001b[0m\n",
       "\u001b[32mscene\\nsynthesis, and super-resolution, while significantly reducing computational\\nrequirements compared to \u001b[0m\n",
       "\u001b[32mpixel-based DMs. Code is available at\\nhttps://github.com/CompVis/latent-diffusion .'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 8\n",
      " - # Chunks: 155\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-02-26'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Learning Transferable Visual Models From Natural Language Supervision'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">object categories. This restricted form of supervision limits\\ntheir generality and usability since additional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">labeled data is needed to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">promising alternative which leverages a much broader source of\\nsupervision. We demonstrate that the simple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training task of predicting\\nwhich caption goes with which image is an efficient and scalable way to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">learn\\nSOTA image representations from scratch on a dataset of 400 million (image,\\ntext) pairs collected from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">internet. After pre-training, natural language\\nis used to reference learned visual concepts (or describe new ones)</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">benchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-02-26'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Learning Transferable Visual Models From Natural Language Supervision'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish \u001b[0m\n",
       "\u001b[32mSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined \u001b[0m\n",
       "\u001b[32mobject categories. This restricted form of supervision limits\\ntheir generality and usability since additional \u001b[0m\n",
       "\u001b[32mlabeled data is needed to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a \u001b[0m\n",
       "\u001b[32mpromising alternative which leverages a much broader source of\\nsupervision. We demonstrate that the simple \u001b[0m\n",
       "\u001b[32mpre-training task of predicting\\nwhich caption goes with which image is an efficient and scalable way to \u001b[0m\n",
       "\u001b[32mlearn\\nSOTA image representations from scratch on a dataset of 400 million \u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage,\\ntext\u001b[0m\u001b[32m)\u001b[0m\u001b[32m pairs collected from the \u001b[0m\n",
       "\u001b[32minternet. After pre-training, natural language\\nis used to reference learned visual concepts \u001b[0m\u001b[32m(\u001b[0m\u001b[32mor describe new ones\u001b[0m\u001b[32m)\u001b[0m\n",
       "\u001b[32menabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by \u001b[0m\n",
       "\u001b[32mbenchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action \u001b[0m\n",
       "\u001b[32mrecognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel \u001b[0m\n",
       "\u001b[32mtransfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need \u001b[0m\n",
       "\u001b[32mfor any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on \u001b[0m\n",
       "\u001b[32mImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release\u001b[0m\n",
       "\u001b[32mour code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 9\n",
      " - # Chunks: 38\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-04-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Reasoning Steps'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Yu Cui, Bryan Hooi, Yujun Cai, Yiwei Wang'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Recent reasoning large language models (LLMs) have demonstrated remarkable\\nimprovements in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mathematical reasoning capabilities through long\\nChain-of-Thought. The reasoning tokens of these models enable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">self-correction\\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\\nhow vulnerable are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning LLMs to subtle errors in their input reasoning\\nchains? We introduce \"Compromising Thought\" (CPT), a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vulnerability where models\\npresented with reasoning tokens containing manipulated calculation results tend\\nto </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ignore correct reasoning steps and adopt incorrect results instead. Through\\nsystematic evaluation across multiple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning LLMs, we design three\\nincreasingly explicit prompting methods to measure CPT resistance, revealing\\nthat</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models struggle significantly to identify and correct these manipulations.\\nNotably, contrary to existing research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suggesting structural alterations affect\\nmodel performance more than content modifications, we find that local </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ending\\ntoken manipulations have greater impact on reasoning outcomes than structural\\nchanges. Moreover, we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">discover a security vulnerability in DeepSeek-R1 where\\ntampered reasoning tokens can trigger complete reasoning </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cessation. Our work\\nenhances understanding of reasoning robustness and highlights security\\nconsiderations for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning-intensive applications.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2025-04-01'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct \u001b[0m\n",
       "\u001b[32mReasoning Steps'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Yu Cui, Bryan Hooi, Yujun Cai, Yiwei Wang'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Recent reasoning large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have demonstrated remarkable\\nimprovements in \u001b[0m\n",
       "\u001b[32mmathematical reasoning capabilities through long\\nChain-of-Thought. The reasoning tokens of these models enable \u001b[0m\n",
       "\u001b[32mself-correction\\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\\nhow vulnerable are\u001b[0m\n",
       "\u001b[32mreasoning LLMs to subtle errors in their input reasoning\\nchains? We introduce \"Compromising Thought\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCPT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, a \u001b[0m\n",
       "\u001b[32mvulnerability where models\\npresented with reasoning tokens containing manipulated calculation results tend\\nto \u001b[0m\n",
       "\u001b[32mignore correct reasoning steps and adopt incorrect results instead. Through\\nsystematic evaluation across multiple \u001b[0m\n",
       "\u001b[32mreasoning LLMs, we design three\\nincreasingly explicit prompting methods to measure CPT resistance, revealing\\nthat\u001b[0m\n",
       "\u001b[32mmodels struggle significantly to identify and correct these manipulations.\\nNotably, contrary to existing research \u001b[0m\n",
       "\u001b[32msuggesting structural alterations affect\\nmodel performance more than content modifications, we find that local \u001b[0m\n",
       "\u001b[32mending\\ntoken manipulations have greater impact on reasoning outcomes than structural\\nchanges. Moreover, we \u001b[0m\n",
       "\u001b[32mdiscover a security vulnerability in DeepSeek-R1 where\\ntampered reasoning tokens can trigger complete reasoning \u001b[0m\n",
       "\u001b[32mcessation. Our work\\nenhances understanding of reasoning robustness and highlights security\\nconsiderations for \u001b[0m\n",
       "\u001b[32mreasoning-intensive applications.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")\n",
    "\n",
    "## TODO: Please pick some papers and add them to the list as you'd like\n",
    "## NOTE: To re-use for the final assessment, make sure at least one paper is < 1 month old\n",
    "print(\"Loading Documents\")\n",
    "docs = [\n",
    "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    ## Some longer papers\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
    "    ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
    "    ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
    "    ArxivLoader(query=\"2503.19326v2\").load(),\n",
    "    # 2503.19326v2\n",
    "    ## TODO: Feel free to add more\n",
    "]\n",
    "\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]\n",
    "\n",
    "## Split the documents and also filter out stubs (overly short chunks)\n",
    "print(\"Chunking Documents\")\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "\n",
    "## Printing out some summary information for reference\n",
    "pprint(doc_string, '\\n')\n",
    "for i, chunks in enumerate(docs_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    pprint(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pWU_OOnnrsT",
   "metadata": {
    "id": "4pWU_OOnnrsT"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2**: Construct Your Document Vector Stores\n",
    "\n",
    "Now that we have all of the components, we can go ahead and create indices surrounding them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lwwmr3aptwCg",
   "metadata": {
    "id": "lwwmr3aptwCg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Vector Stores\n",
      "CPU times: user 1.57 s, sys: 101 ms, total: 1.68 s\n",
      "Wall time: 47.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j39JwCKubto0",
   "metadata": {
    "id": "j39JwCKubto0"
   },
   "source": [
    "<br>\n",
    "\n",
    "From there, we can combine our indices into a single one using the following utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Q7us66iPVc70",
   "metadata": {
    "id": "Q7us66iPVc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 610 chunks\n"
     ]
    }
   ],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VU_VEx2mqJUK",
   "metadata": {
    "id": "VU_VEx2mqJUK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 3: [Exercise]** Implement Your RAG Chain\n",
    "\n",
    "Finally, all the puzzle pieces are in place to implement the RAG pipeline! As a review, we now have:\n",
    "\n",
    "- A way to construct a from-scratch vector store for conversational memory (and a way to initialize an empty one with `default_FAISS()`)\n",
    "\n",
    "- A vector store pre-loaded with useful document information from our `ArxivLoader` utility (stored in `docstore`).\n",
    "\n",
    "With the help of a couple more utilities, you're finally ready to integrate your chain! A few additional convenience utilities are provided (`doc2str` and the now-common `RPrint`) but are optional to use. Additionally, some starter prompts and structures are also defined.\n",
    "\n",
    "> **Given all of this:** Please implement the `retrieval_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-RXSrb1GcNff",
   "metadata": {
    "id": "-RXSrb1GcNff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conversation History Retrieval:\\n\\n\\n Document Retrieval:\\n[Quote from Retrieval-Augmented Generation for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge-Intensive NLP Tasks] . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">target token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">latent variable that\\\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation\\n[Quote from</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] .\\\\nRAG-Token\\\\nThe RAG-Token model can be seen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as a standard, autoregressive seq2seq genera-\\\\ntor with transition probability: p\\\\u2032\\\\n\\\\u03b8(yi|x, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">y1:i\\\\u22121) = P\\\\nz\\\\u2208top-k(p(\\\\u00b7|x)) p\\\\u03b7(zi|x)p\\\\u03b8(yi|x, zi, y1:i\\\\u22121) To\\\\ndecode, we can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">plug p\\\\u2032\\\\n\\\\u03b8(yi|x, y1:i\\\\u22121) into a standard beam decoder.\\\\nRAG-Sequence\\\\nFor RAG-Sequence, the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">likelihood p(y|x) does not break into a conventional per-\\\\ntoken likelihood, hence we cannot solve it with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">single beam search. Instead, we run beam search for\\\\neach document z, scoring each hypothesis using p\\\\u03b8(yi|x,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">z, y1:i\\\\u22121). This yields a set of hypotheses\\\\nY , some of which may not have appeared in the beams of all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documents. To estimate the probability\\\\nof an hypothesis y we run an additional forward pass for each document z </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for which y does not\\\\nappear in the beam, multiply generator probability with p\\\\u03b7(z|x) and then sum the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">probabilities across\\\\nbeams for the marginals\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">NLP Tasks] . Each Wikipedia article is split into disjoint\\\\n100-word chunks, to make a total of 21M documents. We </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">use the document encoder to compute an\\\\nembedding for each document, and build a single MIPS index using FAISS </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[23] with a Hierarchical\\\\nNavigable Small World approximation for fast retrieval [37]. During training, we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve the top\\\\nk documents for each query. We consider k \\\\u2208{5, 10} for training and set k for test time </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using dev\\\\ndata. We now discuss experimental details for each task.\\\\n3.1\\\\nOpen-domain Question </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Answering\\\\nOpen-domain question answering (QA) is an important real-world application and common testbed\\\\nfor </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y)\\\\nand train RAG by</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">directly minimizing the negative log-likelihood of answers. We compare RAG to\\\\nthe popular extractive QA paradigm </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[5, 7, 31, 26], where answers are extracted spans from retrieved\\\\ndocuments, relying primarily on non-parametric </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . Since RAG can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be\\\\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\\\\nto a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lesser extent, including that it might be used to generate abuse, faked or misleading content in\\\\nthe news or on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">social media; to impersonate others; or to automate the production of spam/phishing\\\\ncontent [54]. Advanced </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language models may also lead to the automation of various jobs in the\\\\ncoming decades [16]. In order to mitigate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">these risks, AI systems could be employed to \\\\ufb01ght against\\\\nmisleading content and automated </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">spam/phishing.\\\\nAcknowledgments\\\\nThe authors would like to thank the reviewers for their thoughtful and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constructive feedback on this\\\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models. The authors\\\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">EP\\\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">PhD\\\\nprogram.\\\\n\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . We refer to this</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">decoding procedure as \\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">requiring many forward passes. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">p\\\\u03b8(y|x, zi) \\\\u22480 where y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">additional forward passes once the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\u201cFast Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks. For all experiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al. [31] and\\\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disjoint\\\\n100-word chunks, to make a total of 21M documents\\n\\n\\n (Answer only from retrieval. Only cite sources </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that are used. Make your response conversational.)'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={})</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
       "\u001b[32mjust asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  \u001b[0m\n",
       "\u001b[32mConversation History Retrieval:\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for \u001b[0m\n",
       "\u001b[32mKnowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each \u001b[0m\n",
       "\u001b[32mtarget token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the\u001b[0m\n",
       "\u001b[32mfollowing, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as \u001b[0m\n",
       "\u001b[32mthe training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same \u001b[0m\n",
       "\u001b[32mretrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single \u001b[0m\n",
       "\u001b[32mlatent variable that\\\\nis marginalized to get the seq2seq probability p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m via a top-K approximation\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from\u001b[0m\n",
       "\u001b[32mRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m .\\\\nRAG-Token\\\\nThe RAG-Token model can be seen \u001b[0m\n",
       "\u001b[32mas a standard, autoregressive seq2seq genera-\\\\ntor with transition probability: p\\\\u2032\\\\n\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, \u001b[0m\n",
       "\u001b[32my1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m = P\\\\nz\\\\u2208top-k\u001b[0m\u001b[32m(\u001b[0m\u001b[32mp\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\u00b7|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m p\\\\u03b7\u001b[0m\u001b[32m(\u001b[0m\u001b[32mzi|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32mp\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, zi, y1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m To\\\\ndecode, we can \u001b[0m\n",
       "\u001b[32mplug p\\\\u2032\\\\n\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, y1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m into a standard beam decoder.\\\\nRAG-Sequence\\\\nFor RAG-Sequence, the \u001b[0m\n",
       "\u001b[32mlikelihood p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m does not break into a conventional per-\\\\ntoken likelihood, hence we cannot solve it with a \u001b[0m\n",
       "\u001b[32msingle beam search. Instead, we run beam search for\\\\neach document z, scoring each hypothesis using p\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x,\u001b[0m\n",
       "\u001b[32mz, y1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This yields a set of hypotheses\\\\nY , some of which may not have appeared in the beams of all \u001b[0m\n",
       "\u001b[32mdocuments. To estimate the probability\\\\nof an hypothesis y we run an additional forward pass for each document z \u001b[0m\n",
       "\u001b[32mfor which y does not\\\\nappear in the beam, multiply generator probability with p\\\\u03b7\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and then sum the \u001b[0m\n",
       "\u001b[32mprobabilities across\\\\nbeams for the marginals\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Knowledge-Intensive \u001b[0m\n",
       "\u001b[32mNLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Each Wikipedia article is split into disjoint\\\\n100-word chunks, to make a total of 21M documents. We \u001b[0m\n",
       "\u001b[32muse the document encoder to compute an\\\\nembedding for each document, and build a single MIPS index using FAISS \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m23\u001b[0m\u001b[32m]\u001b[0m\u001b[32m with a Hierarchical\\\\nNavigable Small World approximation for fast retrieval \u001b[0m\u001b[32m[\u001b[0m\u001b[32m37\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. During training, we \u001b[0m\n",
       "\u001b[32mretrieve the top\\\\nk documents for each query. We consider k \\\\u2208\u001b[0m\u001b[32m{\u001b[0m\u001b[32m5, 10\u001b[0m\u001b[32m}\u001b[0m\u001b[32m for training and set k for test time \u001b[0m\n",
       "\u001b[32musing dev\\\\ndata. We now discuss experimental details for each task.\\\\n3.1\\\\nOpen-domain Question \u001b[0m\n",
       "\u001b[32mAnswering\\\\nOpen-domain question answering \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is an important real-world application and common testbed\\\\nfor \u001b[0m\n",
       "\u001b[32mknowledge-intensive tasks \u001b[0m\u001b[32m[\u001b[0m\u001b[32m20\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. We treat questions and answers as input-output text pairs \u001b[0m\u001b[32m(\u001b[0m\u001b[32mx, y\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\\\nand train RAG by\u001b[0m\n",
       "\u001b[32mdirectly minimizing the negative log-likelihood of answers. We compare RAG to\\\\nthe popular extractive QA paradigm \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m5, 7, 31, 26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, where answers are extracted spans from retrieved\\\\ndocuments, relying primarily on non-parametric \u001b[0m\n",
       "\u001b[32mknowledge\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Since RAG can \u001b[0m\n",
       "\u001b[32mbe\\\\nemployed as a language model, similar concerns as for GPT-2 \u001b[0m\u001b[32m[\u001b[0m\u001b[32m50\u001b[0m\u001b[32m]\u001b[0m\u001b[32m are valid here, although arguably\\\\nto a \u001b[0m\n",
       "\u001b[32mlesser extent, including that it might be used to generate abuse, faked or misleading content in\\\\nthe news or on \u001b[0m\n",
       "\u001b[32msocial media; to impersonate others; or to automate the production of spam/phishing\\\\ncontent \u001b[0m\u001b[32m[\u001b[0m\u001b[32m54\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Advanced \u001b[0m\n",
       "\u001b[32mlanguage models may also lead to the automation of various jobs in the\\\\ncoming decades \u001b[0m\u001b[32m[\u001b[0m\u001b[32m16\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In order to mitigate \u001b[0m\n",
       "\u001b[32mthese risks, AI systems could be employed to \\\\ufb01ght against\\\\nmisleading content and automated \u001b[0m\n",
       "\u001b[32mspam/phishing.\\\\nAcknowledgments\\\\nThe authors would like to thank the reviewers for their thoughtful and \u001b[0m\n",
       "\u001b[32mconstructive feedback on this\\\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG \u001b[0m\n",
       "\u001b[32mmodels. The authors\\\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. \u001b[0m\n",
       "\u001b[32mEP\\\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR \u001b[0m\n",
       "\u001b[32mPhD\\\\nprogram.\\\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . We refer to this\u001b[0m\n",
       "\u001b[32mdecoding procedure as \\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, \u001b[0m\n",
       "\u001b[32mrequiring many forward passes. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that \u001b[0m\n",
       "\u001b[32mp\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x, zi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \\\\u22480 where y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run \u001b[0m\n",
       "\u001b[32madditional forward passes once the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \u001b[0m\n",
       "\u001b[32m\\\\u201cFast Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive \u001b[0m\n",
       "\u001b[32mtasks. For all experiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee\u001b[0m\n",
       "\u001b[32met al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m31\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and\\\\nKarpukhin et al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, we use the December 2018 dump. Each Wikipedia article is split into \u001b[0m\n",
       "\u001b[32mdisjoint\\\\n100-word chunks, to make a total of 21M documents\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from retrieval. Only cite sources \u001b[0m\n",
       "\u001b[32mthat are used. Make your response conversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Tell me about RAG!'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG stands for Retrieval-Augmented Generation. It is a method used in Natural Language Processing (NLP) for knowledge-intensive tasks. This technique involves using a retrieval system to find the most relevant documents to a given input, and then generating an output sequence based on the input and the content of the retrieved documents.\n",
      "\n",
      "There are two main approaches to RAG: RAG-Sequence and RAG-Token. In RAG-Sequence, the same retrieved document is used to generate the entire output sequence. On the other hand, RAG-Token can predict each output token based on a different retrieved document.\n",
      "\n",
      "During training, the RAG model retrieves the top k documents for each input sequence using a maximum inner product search (MIPS) index. The documents are then encoded using a document encoder, and the likelihood of the output sequence given the input sequence and the encoded documents is computed using a beam search algorithm.\n",
      "\n",
      "RAG can be used for various knowledge-intensive tasks such as open-domain question answering. In this task, RAG is trained by minimizing the negative log-likelihood of the answers to questions, which are treated as input-output text pairs.\n",
      "\n",
      "Some potential benefits of RAG include improved accuracy and reduced reliance on pre-trained language models, which can be computationally expensive to train and require large amounts of data. However, RAG models can also present some risks, such as the potential to generate abusive or misleading content.\n",
      "\n",
      "In summary, RAG is a retrieval-augmented generation method for knowledge-intensive NLP tasks, which involves retrieving relevant documents and generating output sequences based on the input and the content of the retrieved documents. RAG has the potential to improve accuracy and reduce reliance on pre-trained language models but also presents some risks.Sources:\n",
      "\n",
      "* \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" by Lewis, et al.\n",
      "* \"Ethical and Social Risks of Conversational AI\" by Diamantopoulou and Dickler.\n",
      "* \"The Impact of AI on the Future of Work\" by Brynjolfsson, et al."
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "stream_chain = chat_prompt | instruct_llm | StrOutputParser()\n",
    "\n",
    "################################################################################################\n",
    "## BEGIN TODO: Implement the retrieval chain to make your system work!\n",
    "\n",
    "# First create a retrieval chain that gets both conversation history and document context\n",
    "retrieval_chain = (\n",
    "    {'input': (lambda x: x)}\n",
    "    | RunnableAssign({\n",
    "        'history': itemgetter('input') \n",
    "            | convstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "            | RunnableLambda(LongContextReorder().transform_documents) \n",
    "            | partial(docs2str, title=\"Conversation History\"),\n",
    "        'context': itemgetter('input') \n",
    "            | docstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "            | RunnableLambda(LongContextReorder().transform_documents) \n",
    "            | partial(docs2str, title=\"Document Content\"),\n",
    "    })\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "################################################################################################\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9W7sC5Z6BfqM",
   "metadata": {
    "id": "9W7sC5Z6BfqM"
   },
   "source": [
    "### **Task 4:** Interact With Your Gradio Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fMP3l7QL2JWT",
   "metadata": {
    "id": "fMP3l7QL2JWT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.41.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://fd837d910e4d62b69e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://fd837d910e4d62b69e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: What does it mean by judging LLM as a  judge ? can I use llms as judges ? is that feasible ? LLMs </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hallucinate\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval:\\n[Quote from Conversation Context] User previously responded with Tell me about RAG!\\n[Quote from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conversation Context] Agent previously responded with RAG stands for Retrieval-Augmented Generation. It is a method</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">used in Natural Language Processing (NLP) for knowledge-intensive tasks. This technique involves using a retrieval </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system to find the most relevant documents to a given input, and then generating an output sequence based on the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">input and the content of the retrieved documents.\\n\\nThere are two main approaches to RAG: RAG-Sequence and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG-Token. In RAG-Sequence, the same retrieved document is used to generate the entire output sequence. On the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">other hand, RAG-Token can predict each output token based on a different retrieved document.\\n\\nDuring training, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the RAG model retrieves the top k documents for each input sequence using a maximum inner product search (MIPS) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">index. The documents are then encoded using a document encoder, and the likelihood of the output sequence given the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">input sequence and the encoded documents is computed using a beam search algorithm.\\n\\nRAG can be used for various </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive tasks such as open-domain question answering. In this task, RAG is trained by minimizing the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">negative log-likelihood of the answers to questions, which are treated as input-output text pairs.\\n\\nSome </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">potential benefits of RAG include improved accuracy and reduced reliance on pre-trained language models, which can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be computationally expensive to train and require large amounts of data. However, RAG models can also present some </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">risks, such as the potential to generate abusive or misleading content.\\n\\nIn summary, RAG is a retrieval-augmented</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generation method for knowledge-intensive NLP tasks, which involves retrieving relevant documents and generating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output sequences based on the input and the content of the retrieved documents. RAG has the potential to improve </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accuracy and reduce reliance on pre-trained language models but also presents some risks.Sources:\\n\\n* </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" by Lewis, et al.\\n* \"Ethical and Social Risks of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conversational AI\" by Diamantopoulou and Dickler.\\n* \"The Impact of AI on the Future of Work\" by Brynjolfsson, et </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al.\\n\\n\\n Document Retrieval:\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] . Next, we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">discuss the use and limitations of LLM-as-a-judge.\\\\n3.1\\\\nTypes of LLM-as-a-Judge\\\\nWe propose 3 LLM-as-a-judge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">variations. They can be implemented independently or in combination:\\\\n\\\\u2022 Pairwise comparison. An LLM judge is</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">presented with a question and two answers, and tasked\\\\nto determine which one is better or declare a tie. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompt used is given in Figure 5 (Appendix).\\\\n\\\\u2022 Single answer grading. Alternatively, an LLM judge is asked </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to directly assign a score to a\\\\nsingle answer. The prompt used for this scenario is in Figure 6 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(Appendix).\\\\n\\\\u2022 Reference-guided grading. In certain cases, it may be beneficial to provide a reference </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">solution\\\\nif applicable. An example prompt we use for grading math problems is in Figure 8 (Appendix).\\\\nThese </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">methods have different pros and cons\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">.\\\\n3.3\\\\nLimitations of LLM-as-a-Judge\\\\nWe identify certain biases and limitations of LLM judges. However, we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will also present solutions\\\\nlater and show the agreement between LLM judges and humans is high despite these </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations.\\\\nPosition bias is when an LLM exhibits a propensity to favor certain positions over others. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">This\\\\nbias is not unique to our context and has been seen in human decision-making [3, 34] and other ML\\\\ndomains </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[22, 41].\\\\nFigure 11 (Appendix) shows an example of position bias. GPT-4 is tasked to evaluate two </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">responses\\\\nfrom GPT-3.5 and Vicuna-13B to an open-ended question. When GPT-3.5\\\\u2019s answer is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">positioned\\\\n4\\\\nTable 2: Position bias of different LLM judges. Consistency is the percentage of cases where </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a\\\\njudge gives consistent results when swapping the order of two assistants. \\\\u201cBiased toward first\\\\u201d is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the\\\\npercentage of cases when a judge favors the first answer. \\\\u201cError\\\\u201d indicates wrong output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">formats\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] . Detailed\\\\nprompt in Figure 7 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(Appendix). However, even with the CoT prompt, we find that in many cases\\\\nLLM makes exactly the same mistake as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the given answers in its problem-solving process (See\\\\nexample in Figure 15 (Appendix), suggesting that LLM judge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">may still be misled by the context.\\\\nHence, we propose a reference-guided method, in which we first generate LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">judge\\\\u2019s answer\\\\nindependently, and then display it as a reference answer in the judge prompt. In Table 4, we</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">see a\\\\nsignificant improvement in failure rate (from 70% to 15%) over the default prompt.\\\\nFine-tuning a judge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model. We try fine-tuning a Vicuna-13B on arena data to act as a judge and\\\\nshow some promising preliminary </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results in Appendix F.\\\\n3.5\\\\nMulti-turn judge\\\\nIn MT-bench, every question involves two turns to evaluate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversational abilities. Therefore, when\\\\ncomparing two assistants, it becomes necessary to present a total of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">two questions and four responses,\\\\ncomplicating the prompt design\\n[Quote from Judging LLM-as-a-Judge with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">MT-Bench and Chatbot Arena] . We then verify the agreement between LLM judges and human preferences\\\\nby </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot\\\\nArena, a crowdsourced battle </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">platform. Our results reveal that strong LLM judges\\\\nlike GPT-4 can match both controlled and crowdsourced human </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preferences well,\\\\nachieving over 80% agreement, the same level of agreement between humans.\\\\nHence, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM-as-a-judge is a scalable and explainable way to approximate human\\\\npreferences, which are otherwise very </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expensive to obtain. Additionally, we show\\\\nour benchmark and traditional benchmarks complement each other by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">evaluating\\\\nseveral variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes,\\\\nand 30K conversations</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with human preferences are publicly available at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https:\\\\n//github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge\\n[Quote from Judging LLM-as-a-Judge with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">MT-Bench and Chatbot Arena] .\\\\nThese methods have different pros and cons. For example, the pairwise comparison </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">may lack\\\\nscalability when the number of players increases, given that the number of possible pairs </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">grows\\\\nquadratically; single answer grading may be unable to discern subtle differences between specific\\\\npairs, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and its results may become unstable, as absolute scores are likely to fluctuate more than relative\\\\npairwise </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results if the judge model changes.\\\\n3.2\\\\nAdvantages of LLM-as-a-Judge\\\\nLLM-as-a-judge offers two key benefits: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scalability and explainability. It reduces the need for human\\\\ninvolvement, enabling scalable benchmarks and fast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">iterations. Additionally, LLM judges provide\\\\nnot only scores but also explanations, making their outputs </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interpretable, as shown in Figure 1.\\\\n3.3\\\\nLimitations of LLM-as-a-Judge\\\\nWe identify certain biases and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations of LLM judges\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversational.)'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'What does it mean by judging LLM as a  judge ? can I use llms as judges ? is that feasible ? </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLMs hallucinate'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
       "\u001b[32mjust asked: What does it mean by judging LLM as a  judge ? can I use llms as judges ? is that feasible ? LLMs \u001b[0m\n",
       "\u001b[32mhallucinate\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History \u001b[0m\n",
       "\u001b[32mRetrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Conversation Context\u001b[0m\u001b[32m]\u001b[0m\u001b[32m User previously responded with Tell me about RAG!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from \u001b[0m\n",
       "\u001b[32mConversation Context\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Agent previously responded with RAG stands for Retrieval-Augmented Generation. It is a method\u001b[0m\n",
       "\u001b[32mused in Natural Language Processing \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNLP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for knowledge-intensive tasks. This technique involves using a retrieval \u001b[0m\n",
       "\u001b[32msystem to find the most relevant documents to a given input, and then generating an output sequence based on the \u001b[0m\n",
       "\u001b[32minput and the content of the retrieved documents.\\n\\nThere are two main approaches to RAG: RAG-Sequence and \u001b[0m\n",
       "\u001b[32mRAG-Token. In RAG-Sequence, the same retrieved document is used to generate the entire output sequence. On the \u001b[0m\n",
       "\u001b[32mother hand, RAG-Token can predict each output token based on a different retrieved document.\\n\\nDuring training, \u001b[0m\n",
       "\u001b[32mthe RAG model retrieves the top k documents for each input sequence using a maximum inner product search \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMIPS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mindex. The documents are then encoded using a document encoder, and the likelihood of the output sequence given the\u001b[0m\n",
       "\u001b[32minput sequence and the encoded documents is computed using a beam search algorithm.\\n\\nRAG can be used for various \u001b[0m\n",
       "\u001b[32mknowledge-intensive tasks such as open-domain question answering. In this task, RAG is trained by minimizing the \u001b[0m\n",
       "\u001b[32mnegative log-likelihood of the answers to questions, which are treated as input-output text pairs.\\n\\nSome \u001b[0m\n",
       "\u001b[32mpotential benefits of RAG include improved accuracy and reduced reliance on pre-trained language models, which can \u001b[0m\n",
       "\u001b[32mbe computationally expensive to train and require large amounts of data. However, RAG models can also present some \u001b[0m\n",
       "\u001b[32mrisks, such as the potential to generate abusive or misleading content.\\n\\nIn summary, RAG is a retrieval-augmented\u001b[0m\n",
       "\u001b[32mgeneration method for knowledge-intensive NLP tasks, which involves retrieving relevant documents and generating \u001b[0m\n",
       "\u001b[32moutput sequences based on the input and the content of the retrieved documents. RAG has the potential to improve \u001b[0m\n",
       "\u001b[32maccuracy and reduce reliance on pre-trained language models but also presents some risks.Sources:\\n\\n* \u001b[0m\n",
       "\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" by Lewis, et al.\\n* \"Ethical and Social Risks of\u001b[0m\n",
       "\u001b[32mConversational AI\" by Diamantopoulou and Dickler.\\n* \"The Impact of AI on the Future of Work\" by Brynjolfsson, et \u001b[0m\n",
       "\u001b[32mal.\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Next, we \u001b[0m\n",
       "\u001b[32mdiscuss the use and limitations of LLM-as-a-judge.\\\\n3.1\\\\nTypes of LLM-as-a-Judge\\\\nWe propose 3 LLM-as-a-judge \u001b[0m\n",
       "\u001b[32mvariations. They can be implemented independently or in combination:\\\\n\\\\u2022 Pairwise comparison. An LLM judge is\u001b[0m\n",
       "\u001b[32mpresented with a question and two answers, and tasked\\\\nto determine which one is better or declare a tie. The \u001b[0m\n",
       "\u001b[32mprompt used is given in Figure 5 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAppendix\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\\\n\\\\u2022 Single answer grading. Alternatively, an LLM judge is asked \u001b[0m\n",
       "\u001b[32mto directly assign a score to a\\\\nsingle answer. The prompt used for this scenario is in Figure 6 \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mAppendix\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\\\n\\\\u2022 Reference-guided grading. In certain cases, it may be beneficial to provide a reference \u001b[0m\n",
       "\u001b[32msolution\\\\nif applicable. An example prompt we use for grading math problems is in Figure 8 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAppendix\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\\\nThese \u001b[0m\n",
       "\u001b[32mmethods have different pros and cons\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m.\\\\n3.3\\\\nLimitations of LLM-as-a-Judge\\\\nWe identify certain biases and limitations of LLM judges. However, we \u001b[0m\n",
       "\u001b[32mwill also present solutions\\\\nlater and show the agreement between LLM judges and humans is high despite these \u001b[0m\n",
       "\u001b[32mlimitations.\\\\nPosition bias is when an LLM exhibits a propensity to favor certain positions over others. \u001b[0m\n",
       "\u001b[32mThis\\\\nbias is not unique to our context and has been seen in human decision-making \u001b[0m\u001b[32m[\u001b[0m\u001b[32m3, 34\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and other ML\\\\ndomains \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m22, 41\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\\\nFigure 11 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAppendix\u001b[0m\u001b[32m)\u001b[0m\u001b[32m shows an example of position bias. GPT-4 is tasked to evaluate two \u001b[0m\n",
       "\u001b[32mresponses\\\\nfrom GPT-3.5 and Vicuna-13B to an open-ended question. When GPT-3.5\\\\u2019s answer is \u001b[0m\n",
       "\u001b[32mpositioned\\\\n4\\\\nTable 2: Position bias of different LLM judges. Consistency is the percentage of cases where \u001b[0m\n",
       "\u001b[32ma\\\\njudge gives consistent results when swapping the order of two assistants. \\\\u201cBiased toward first\\\\u201d is \u001b[0m\n",
       "\u001b[32mthe\\\\npercentage of cases when a judge favors the first answer. \\\\u201cError\\\\u201d indicates wrong output \u001b[0m\n",
       "\u001b[32mformats\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Detailed\\\\nprompt in Figure 7 \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mAppendix\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. However, even with the CoT prompt, we find that in many cases\\\\nLLM makes exactly the same mistake as \u001b[0m\n",
       "\u001b[32mthe given answers in its problem-solving process \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSee\\\\nexample in Figure 15 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAppendix\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, suggesting that LLM judge \u001b[0m\n",
       "\u001b[32mmay still be misled by the context.\\\\nHence, we propose a reference-guided method, in which we first generate LLM \u001b[0m\n",
       "\u001b[32mjudge\\\\u2019s answer\\\\nindependently, and then display it as a reference answer in the judge prompt. In Table 4, we\u001b[0m\n",
       "\u001b[32msee a\\\\nsignificant improvement in failure rate \u001b[0m\u001b[32m(\u001b[0m\u001b[32mfrom 70% to 15%\u001b[0m\u001b[32m)\u001b[0m\u001b[32m over the default prompt.\\\\nFine-tuning a judge \u001b[0m\n",
       "\u001b[32mmodel. We try fine-tuning a Vicuna-13B on arena data to act as a judge and\\\\nshow some promising preliminary \u001b[0m\n",
       "\u001b[32mresults in Appendix F.\\\\n3.5\\\\nMulti-turn judge\\\\nIn MT-bench, every question involves two turns to evaluate \u001b[0m\n",
       "\u001b[32mconversational abilities. Therefore, when\\\\ncomparing two assistants, it becomes necessary to present a total of \u001b[0m\n",
       "\u001b[32mtwo questions and four responses,\\\\ncomplicating the prompt design\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Judging LLM-as-a-Judge with \u001b[0m\n",
       "\u001b[32mMT-Bench and Chatbot Arena\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . We then verify the agreement between LLM judges and human preferences\\\\nby \u001b[0m\n",
       "\u001b[32mintroducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot\\\\nArena, a crowdsourced battle \u001b[0m\n",
       "\u001b[32mplatform. Our results reveal that strong LLM judges\\\\nlike GPT-4 can match both controlled and crowdsourced human \u001b[0m\n",
       "\u001b[32mpreferences well,\\\\nachieving over 80% agreement, the same level of agreement between humans.\\\\nHence, \u001b[0m\n",
       "\u001b[32mLLM-as-a-judge is a scalable and explainable way to approximate human\\\\npreferences, which are otherwise very \u001b[0m\n",
       "\u001b[32mexpensive to obtain. Additionally, we show\\\\nour benchmark and traditional benchmarks complement each other by \u001b[0m\n",
       "\u001b[32mevaluating\\\\nseveral variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes,\\\\nand 30K conversations\u001b[0m\n",
       "\u001b[32mwith human preferences are publicly available at \u001b[0m\n",
       "\u001b[32mhttps:\\\\n//github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Judging LLM-as-a-Judge with \u001b[0m\n",
       "\u001b[32mMT-Bench and Chatbot Arena\u001b[0m\u001b[32m]\u001b[0m\u001b[32m .\\\\nThese methods have different pros and cons. For example, the pairwise comparison \u001b[0m\n",
       "\u001b[32mmay lack\\\\nscalability when the number of players increases, given that the number of possible pairs \u001b[0m\n",
       "\u001b[32mgrows\\\\nquadratically; single answer grading may be unable to discern subtle differences between specific\\\\npairs, \u001b[0m\n",
       "\u001b[32mand its results may become unstable, as absolute scores are likely to fluctuate more than relative\\\\npairwise \u001b[0m\n",
       "\u001b[32mresults if the judge model changes.\\\\n3.2\\\\nAdvantages of LLM-as-a-Judge\\\\nLLM-as-a-judge offers two key benefits: \u001b[0m\n",
       "\u001b[32mscalability and explainability. It reduces the need for human\\\\ninvolvement, enabling scalable benchmarks and fast \u001b[0m\n",
       "\u001b[32miterations. Additionally, LLM judges provide\\\\nnot only scores but also explanations, making their outputs \u001b[0m\n",
       "\u001b[32minterpretable, as shown in Figure 1.\\\\n3.3\\\\nLimitations of LLM-as-a-Judge\\\\nWe identify certain biases and \u001b[0m\n",
       "\u001b[32mlimitations of LLM judges\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from retrieval. Only cite sources that are used. Make your response \u001b[0m\n",
       "\u001b[32mconversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'What does it mean by judging LLM as a  judge ? can I use llms as judges ? is that feasible ? \u001b[0m\n",
       "\u001b[32mLLMs hallucinate'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://fd837d910e4d62b69e.gradio.live\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCb3RVVfbmQ0",
   "metadata": {
    "id": "yCb3RVVfbmQ0"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Saving Your Index For Evaluation\n",
    "\n",
    "After you've implemented your RAG chain, please save your accumulated vector store as shown [in the official documentation](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading). You'll have a chance to use it again for your final assessment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "Y4se5wQ4Afda",
   "metadata": {
    "id": "Y4se5wQ4Afda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    }
   ],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"docstore_index\")\n",
    "!tar czvf docstore_index.tgz docstore_index\n",
    "\n",
    "!rm -rf docstore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LsI7NivbIgFw",
   "metadata": {
    "id": "LsI7NivbIgFw"
   },
   "source": [
    "If everything was properly saved, the following line can be invoked to pull the index from the compressed `tgz` file (assuming the pip requirements are installed). After you have confirmed that the cell can pull in your index, download `docstore_index.tgz` for use in the last notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "Qs8820ucIu1t",
   "metadata": {
    "id": "Qs8820ucIu1t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n",
      ". To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7\\nTable 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality\\nSpeci\\ufb01city\\nBART better\\n7.1%\\n16.8%\\nRAG better\\n42.7%\\n37.4%\\nBoth good\\n11.7%\\n11.8%\\nBoth poor\\n17.7%\\n6.9%\\nNo majority\\n20.8%\\n20.1%\\nTable 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO\\nJeopardy QGen\\nGold\\n89.6%\\n90.0%\\nBART\\n70.7%\\n32.4%\\nRAG-Token\\n77.8%\\n46.8%\\nRAG-Seq.\\n83.5%\\n53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classi\\ufb01cation task, both RAG models are equivalent.\\nModel\\nNQ\\nTQA\\nWQ\\nCT\\nJeopardy-QGen\\nMSMarco\\nFVR-3\\nFVR-2\\nExact Match\\nB-1\\nQB-1\\nR-L\\nB-1\\nLabel Accuracy\\nRAG-Token-BM25\\n29.7\\n41.5\\n32.1\\n33.1\\n17.5\\n22.3\\n55.5\\n48.4\\n75.1\\n91.6\\nRAG-Sequence-BM25\\n31.8\\n44.1\\n36.6\\n33\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "!tar xzvf docstore_index.tgz\n",
    "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "as_3vWJGKB2F",
   "metadata": {
    "id": "as_3vWJGKB2F"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5:** Wrap-Up\n",
    "\n",
    "Congratulations! Assuming your RAG chain is all good, you're now ready to move on to the **RAG Evaluation [Assessment]** section!\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4",
   "metadata": {
    "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
